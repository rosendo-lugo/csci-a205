{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002f876-12e4-41a2-a059-1e21c850b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class SpamDetector:\n",
    "    def __init__(self, spam_file):\n",
    "        self.spam_words = self.get_keywords(spam_file)\n",
    "        # Optional: Initialize stop_words\n",
    "\n",
    "    def get_keywords(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            return f.read().split('\\n')\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower().strip().replace('\\n', '')\n",
    "        return re.sub(r'[^a-z0-9\\'\\w\\s]', '', text)\n",
    "\n",
    "    def calculate_spam_score(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        # Optional: text = self.remove_stop_words(text)\n",
    "        spam_points, ham_points = 0, 0\n",
    "        for word in text.split():\n",
    "            if word in self.spam_words:\n",
    "                spam_points += 1\n",
    "            else:\n",
    "                ham_points += 1\n",
    "        return spam_points, ham_points\n",
    "\n",
    "    def classify_email(self, email_text):\n",
    "        spam_points, ham_points = self.calculate_spam_score(email_text)\n",
    "        score = spam_points / max(ham_points, 1)  # Avoid division by zero\n",
    "        if score <= 0.2:\n",
    "            return \"Likely not spam\"\n",
    "        elif score <= 0.4:\n",
    "            return \"Possible spam\"\n",
    "        else:\n",
    "            return \"Likely spam\"\n",
    "\n",
    "    # Additional methods like remove_stop_words can be added here.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detector = SpamDetector('spam_words.txt')\n",
    "    email = \"\"\"Urgent! \\nPlease verify your bank account by\n",
    "    clicking the link: ACTION REQUIRED. Please verify your\n",
    "    Bank of America account information to avoid a hold on\n",
    "    your account. Click here to confirm: [Link]\"\"\"\n",
    "    print(detector.classify_email(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79756c4-0bac-4dd4-a018-f195eeb9b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class SpamDetector:\n",
    "    def __init__(self, spam_file, stop_words_file=None):\n",
    "        self.spam_words = self.get_keywords(spam_file)\n",
    "        self.stop_words = self.get_keywords(stop_words_file) if stop_words_file else []\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def get_keywords(self, file):\n",
    "        if file and os.path.exists(file):\n",
    "            with open(file, 'r') as f:\n",
    "                return set(f.read().split('\\n'))\n",
    "        return set()\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower().strip().replace('\\n', '')\n",
    "        text = re.sub(r'[^a-z0-9\\'\\w\\s]', '', text)\n",
    "        return ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def calculate_spam_score(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        spam_points, ham_points = 0, 0\n",
    "        for word in text.split():\n",
    "            if word in self.spam_words:\n",
    "                spam_points += 1\n",
    "            else:\n",
    "                ham_points += 1\n",
    "        return spam_points, ham_points\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def classify_email(self, email_text):\n",
    "        spam_points, ham_points = self.calculate_spam_score(email_text)\n",
    "        score = spam_points / max(ham_points, 1)  # Avoid division by zero\n",
    "        if score <= 0.2:\n",
    "            return \"Likely not spam\"\n",
    "        elif score <= 0.4:\n",
    "            return \"Possible spam\"\n",
    "        else:\n",
    "            return \"Likely spam\"\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    *------------------*\n",
    "    |                  |\n",
    "    |     PREPARE      |\n",
    "    |                  |\n",
    "    *------------------*\n",
    "    '''\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = self.basic_clean(text)\n",
    "        text = self.tokenize(text)\n",
    "        text = self.lemmatize(text)  # or self.stem(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Redefine calculate_spam_score to use preprocess_text\n",
    "    def calculate_spam_score(self, text):\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def basic_clean(string):\n",
    "        \"\"\"\n",
    "        Lower Case:\n",
    "        - setting all letters to a lowercase\n",
    "\n",
    "        Encoding:\n",
    "        - `unicodedata.normalize` removes any inconsistencies in unicode character encoding\n",
    "        - `.encode` to convert the resulting string to the ASCII character set\n",
    "        - `.decode` to turn the resulting bytes object back into a string\n",
    "\n",
    "        Special characters:\n",
    "        - remove anything that isn't a-z, a number, a single quote, or a whitespace\n",
    "        \"\"\"\n",
    "        # lowercase text\n",
    "        string = string.lower()\n",
    "\n",
    "        # remove any accented characters and non-ASCII characters\n",
    "        # normalizing\n",
    "        # getting ride of anything not in ascii\n",
    "        # turning back to a string\n",
    "        string = unicodedata.normalize('NFKD', string).encode('ascii','ignore').decode('utf-8')\n",
    "\n",
    "        # remove special characters\n",
    "        #use re.sub to remove special characters\n",
    "        bc_string = re.sub(r'[^a-z0-9\\'\\s]', '', string)\n",
    "\n",
    "        return bc_string\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def tokenize(string):\n",
    "        \"\"\"\n",
    "        Tokenization is the process of breaking something down\n",
    "        into smaller, discrete units. These units are called tokens.\n",
    "\n",
    "        It's common to tokenize the strings to break up words and punctutation\n",
    "        left over into discrete units. \n",
    "        \"\"\"  \n",
    "\n",
    "        #create the tokenizer\n",
    "        tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "        tok_string = tokenize.tokenize(string, return_str=True)\n",
    "\n",
    "        return tok_string\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def stem(string):\n",
    "        \"\"\"\n",
    "        Stemming:\n",
    "        - **truncates** words to their \"stem\"\n",
    "        - algorithmic rules (non lingustic)\n",
    "        - example: \"calls\", \"called\", \"calling\" --> \"call\"\n",
    "        - fast and efficient\n",
    "        \"\"\"   \n",
    "        #create porter stemmer\n",
    "        ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "        #use stemmer - apply stem to each word in our string\n",
    "        ps.stem(string)\n",
    "\n",
    "        # split all the words in the article\n",
    "        string.split()\n",
    "        stems = [ps.stem (word) for word in string.split()]\n",
    "\n",
    "        #join words back together\n",
    "        string_stemmed = ' '.join(stems)\n",
    "\n",
    "        return string_stemmed\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def lemmatize(string):\n",
    "        \"\"\"\n",
    "        Lemmatize:\n",
    "            - **changes** words to their \"root\"\n",
    "            - it can conjugate to the base word \n",
    "            - example: \"mouse\", \"mice\" --> \"mouse\"\n",
    "            - slower than stemming\n",
    "        \"\"\" \n",
    "        #create the lemmatizer   \n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        #use lemmatize - apply stem to each word in our string\n",
    "        # wnl.lemmatize(article)\n",
    "        lemma = [wnl.lemmatize(word) for word in string.split()]\n",
    "\n",
    "        #join words back together\n",
    "        string_lemma = ' '.join(lemma)\n",
    "\n",
    "        return string_lemma\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def remove_stopwords(string, string_lemma):\n",
    "        \"\"\"\n",
    "        Words which have little or no significance, especially when constructing\n",
    "        meaningful features from text, are known as stopwords.\n",
    "        - example: a, an, the, and like\n",
    "\n",
    "        We will use a standard English language stopwords list from nltk\n",
    "        \"\"\"\n",
    "        #save stopwords\n",
    "        stopwords_ls = stopwords.words('english')\n",
    "\n",
    "        #split words in lemmatized article\n",
    "        words = string_lemma.split()\n",
    "\n",
    "        #remove stopwords from list of words\n",
    "        filtered = [word for word in words if word not in stopwords_ls]\n",
    "\n",
    "        #join words back together\n",
    "        rem_stopwords = ' '.join(filtered)\n",
    "\n",
    "        return rem_stopwords\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def remove_stopwords_extra_words(string_lemma, extra_words, exclude_words):\n",
    "        \"\"\"\n",
    "        Words which have little or no significance, especially when constructing\n",
    "        meaningful features from text, are known as stopwords.\n",
    "        - example: a, an, the, and like\n",
    "\n",
    "        We will use a standard English language stopwords list from nltk\n",
    "        \"\"\"\n",
    "        #save stopwords\n",
    "        stopwords_ls = stopwords.words('english')\n",
    "\n",
    "        # remove extra words\n",
    "        stopwords_ls = set(stopwords_ls) - set(exclude_words)\n",
    "\n",
    "        # add to stopword list\n",
    "        stopwords_ls = set(stopwords_ls).union(extra_words)\n",
    "\n",
    "        #split words in lemmatized article\n",
    "        words = string_lemma.split()\n",
    "\n",
    "        #remove stopwords from list of words\n",
    "        filtered = [word for word in words if word not in stopwords_ls]\n",
    "\n",
    "        #join words back together\n",
    "        rem_stopwords = ' '.join(filtered)\n",
    "\n",
    "        return rem_stopwords\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    ADDITIONAL_STOPWORDS = ['r', 'u', '2', '4', 'ltgt']\n",
    "\n",
    "    def clean(text):\n",
    "        '''\n",
    "        A simple function to cleanup text data.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of lemmatized words after cleaning.\n",
    "        '''\n",
    "\n",
    "        # basic_clean() function from last lesson:\n",
    "        # Normalize text by removing diacritics, encoding to ASCII, decoding to UTF-8, and converting to lowercase\n",
    "        text = (unicodedata.normalize('NFKD', text)\n",
    "                 .encode('ascii', 'ignore')\n",
    "                 .decode('utf-8', 'ignore')\n",
    "                 .lower())\n",
    "\n",
    "        # Remove punctuation, split text into words\n",
    "        words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "\n",
    "        # Initialize WordNet lemmatizer\n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        # Combine standard English stopwords with additional stopwords\n",
    "        stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "\n",
    "        # Lemmatize words and remove stopwords\n",
    "        cleaned_words = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "        return cleaned_words\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detector = SpamDetector('spam_words.txt', 'stop_words.txt')\n",
    "    email = \"\"\"Urgent! \\nPlease verify your bank account by\n",
    "    clicking the link: ACTION REQUIRED. Please verify your\n",
    "    Bank of America account information to avoid a hold on\n",
    "    your account. Click here to confirm: [Link]\"\"\"\n",
    "    print(detector.classify_email(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611aac25-12f1-4e9f-9f91-6fce701fdbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['urgent', 'please', 'verify', 'bank', 'account', 'clicking', 'link', 'action', 'required', 'please', 'verify', 'bank', 'america', 'account', 'information', 'avoid', 'hold', 'account', 'click', 'confirm', 'link']\n",
      "Original Text Words: ['urgent', 'please', 'verify', 'your', 'bank', 'account', 'by', 'clicking', 'the', 'link', 'action', 'required', 'please', 'verify', 'your', 'bank', 'of', 'america', 'account', 'information', 'to', 'avoid', 'a', 'hold', 'on', 'your', 'account', 'click', 'here', 'to', 'confirm', 'link']\n",
      "Preprocessed Text Words: ['urgent', 'please', 'verify', 'bank', 'account', 'clicking', 'link', 'action', 'required', 'please', 'verify', 'bank', 'america', 'account', 'information', 'avoid', 'hold', 'account', 'click', 'confirm', 'link']\n",
      "Spam Points: 0\n",
      "Ham Points: 32\n",
      "Spam Ratio: 0.00\n",
      "Ham Ratio: 1.00\n",
      "Likely not spam\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class SpamDetector:\n",
    "    def __init__(self, spam_file):\n",
    "        self.spam_words = self.get_keywords(spam_file)\n",
    "\n",
    "    def get_keywords(self, file):\n",
    "        if file and os.path.exists(file):\n",
    "            with open(file, 'r') as f:\n",
    "                return {word.lower() for word in f.read().split('\\n') if word}\n",
    "        return set()\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower().strip().replace('\\n', '')\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "        text = re.sub(r'[^a-z0-9\\'\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def display_sample_spam_words(self):\n",
    "        print(\"Sample Spam Words:\", list(self.spam_words)[:10])  # Display first 10 spam words\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        words = text.split()\n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "        lemmatized_words = [wnl.lemmatize(word) for word in words if word not in stopwords_set]\n",
    "        print(lemmatized_words)\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def calculate_spam_score(self, text):\n",
    "        original_text = self.clean_text(text).split()\n",
    "        preprocessed_text = self.preprocess_text(text).split()\n",
    "\n",
    "        print(\"Original Text Words:\", original_text)\n",
    "        print(\"Preprocessed Text Words:\", preprocessed_text)\n",
    "\n",
    "        spam_points, ham_points = 0, 0\n",
    "\n",
    "        # Decide whether to use original_text or preprocessed_text here\n",
    "        for word in original_text:  # or preprocessed_text\n",
    "            if word in self.spam_words:\n",
    "                spam_points += 1\n",
    "                print(f\"Spam Word Detected: {word}\")\n",
    "            else:\n",
    "                ham_points += 1\n",
    "\n",
    "        # Calculate spam and ham ratios\n",
    "        total_words = len(original_text)  # or len(preprocessed_text)\n",
    "        spam_ratio = spam_points / max(total_words, 1)\n",
    "        ham_ratio = ham_points / max(total_words, 1)\n",
    "\n",
    "        return spam_points, ham_points, spam_ratio, ham_ratio\n",
    "\n",
    "\n",
    "\n",
    "    def classify_email(self, email_text):\n",
    "        spam_points, ham_points, spam_ratio, ham_ratio = self.calculate_spam_score(email_text)\n",
    "        score = spam_points / max(ham_points, 1)\n",
    "        \n",
    "        # Display metrics\n",
    "        print(f\"Spam Points: {spam_points}\")\n",
    "        print(f\"Ham Points: {ham_points}\")\n",
    "        print(f\"Spam Ratio: {spam_ratio:.2f}\")\n",
    "        print(f\"Ham Ratio: {ham_ratio:.2f}\")\n",
    "\n",
    "        # Classification\n",
    "        if score <= 0.2:\n",
    "            return \"Likely not spam\"\n",
    "        elif score <= 0.4:\n",
    "            return \"Possible spam\"\n",
    "        else:\n",
    "            return \"Likely spam\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    detector = SpamDetector('spam_words.txt')\n",
    "    email = \"\"\"Urgent! \\nPlease verify your bank account by\n",
    "    clicking the link: ACTION REQUIRED. Please verify your\n",
    "    Bank of America account information to avoid a hold on\n",
    "    your account. Click here to confirm: [Link]\"\"\"\n",
    "    print(detector.classify_email(email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69eb620-299c-44be-a1af-2d96dadd219f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
